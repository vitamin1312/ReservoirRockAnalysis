{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130fb4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "from utils import read_img\n",
    "import json\n",
    "import gc\n",
    "\n",
    "from segmentation_models_pytorch.losses import DiceLoss, FocalLoss\n",
    "from BlissLearn import BlissLearner\n",
    "from BlissLearn.BlissCallbacks.Callbacks import SegmentationMetricsCallback, PrintCriteriaCallback\n",
    "from utils import calculate_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14102729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b247ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_file = r\"C:\\Users\\Viktor\\Documents\\IT\\ReservoirRockAnalysis\\src\\metadata\\rgb_colors.json\"\n",
    "porosty_file = r\"C:\\Users\\Viktor\\Documents\\IT\\ReservoirRockAnalysis\\src\\metadata\\porosty_info.json\"\n",
    "\n",
    "colors = load_json(colors_file)\n",
    "porosty = load_json(porosty_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa53ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(porosty[0]['classes'])\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0fb554",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2 \n",
    "SIZE = 1024\n",
    "\n",
    "\n",
    "path_to_train = r'C:\\Users\\Viktor\\Documents\\IT\\ReservoirRockAnalysis\\data\\train-test\\segmentation-train-data.xlsx'\n",
    "path_to_test = r'C:\\Users\\Viktor\\Documents\\IT\\ReservoirRockAnalysis\\data\\train-test\\segmentation-test-data.xlsx'\n",
    "path_to_models = r'C:\\Users\\Viktor\\Documents\\IT\\ReservoirRockAnalysis\\resources'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0fe499",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel(path_to_train, index_col=0)\n",
    "train_df['n'] = train_df[[f'class{i}' for i in range(NUM_CLASSES)]].sum(1)\n",
    "train_df = train_df.sort_values(by='n') \n",
    "train_df = train_df.iloc[-240:].copy()\n",
    "train_df.index = np.arange(len(train_df))\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_excel(path_to_test, index_col=0)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc7d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.array([\n",
    "    [0, 0, 0],        # class 0\n",
    "    [0, 255, 0],      # class 1\n",
    "    [255, 0, 255],    # class 2\n",
    "    [255, 255, 0],    # class 3\n",
    "    [255, 0, 0],      # class 4\n",
    "    [0, 255, 255],    # class 5\n",
    "    [255, 255, 255]   # fallback for unknown class\n",
    "])\n",
    "\n",
    "def get_image_mask(mask):\n",
    "\n",
    "    # Клипим значения классов к максимально допустимым (вдруг в маске есть класс 6+)\n",
    "    mask_clipped = np.clip(mask, 0, len(colors) - 1)\n",
    "    \n",
    "    # Применяем векторно цвета\n",
    "    return colors[mask_clipped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_train(SIZE):\n",
    "    return A.Compose(\n",
    "        transforms=[\n",
    "            ToTensorV2(p=1)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def transform_valid(SIZE):\n",
    "    return A.Compose(\n",
    "        transforms=[\n",
    "            A.RandomCrop(SIZE, SIZE, p=1),\n",
    "            ToTensorV2(p=1)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ed0cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, df, transforms):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        image = read_img(self.df.loc[index, 'image_path'])\n",
    "        mask = read_img(self.df.loc[index, 'mask_path'], rgb=False)\n",
    "\n",
    "        transformed = self.transforms(image=image, mask=mask)\n",
    "        image, mask = transformed['image'] / 255, transformed['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7edb8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = SegmentationDataset(train_df, transforms=transform_train(SIZE))\n",
    "valid_datasets = SegmentationDataset(test_df, transforms=transform_valid(SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa0a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_datasets[0]\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d529e1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axs[0].imshow(x.permute(1,2,0).cpu().numpy());\n",
    "axs[1].imshow(get_image_mask(y.cpu().numpy()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c5b5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "\n",
    "    inputs = torch.stack(inputs).to(dtype=torch.float, device=device)\n",
    "    targets = torch.stack(targets).to(dtype=torch.long, device=device)\n",
    "\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16866ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_datasets,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_datasets,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedc498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(train_loader))\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d635f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colorization_model = smp.Unet(\n",
    "#         \"mit_b3\",\n",
    "#         activation='sigmoid',\n",
    "#         in_channels=1,\n",
    "#         classes=3\n",
    "#     ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a57bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colorization_model.load_state_dict(torch.load(r\"C:\\Users\\Viktor\\Documents\\IT\\ReservoirRockAnalysis\\resources\\ColorUnetMIT.pkl\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31b1fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "        \"mit_b3\",\n",
    "        activation=None,\n",
    "        in_channels=3,\n",
    "        classes=NUM_CLASSES,\n",
    "        decoder_attention_type='scse'\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9297d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf4fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(r\"C:\\Users\\Viktor\\Documents\\IT\\ReservoirRockAnalysis\\resources\\UnetMITSegmentationModel.pkl\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd86d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_colorization_input(model, target_in_channels=3):\n",
    "    # Get original conv layer\n",
    "    old_conv = model.encoder.patch_embed1.proj  # Conv2d(1, 64, ...)\n",
    "    old_weights = old_conv.weight  # Shape: (64, 1, 7, 7)\n",
    "\n",
    "    # Repeat or expand weights to match new input channels\n",
    "    new_weights = old_weights.repeat(1, target_in_channels, 1, 1) / target_in_channels\n",
    "\n",
    "    # Replace layer\n",
    "    model.encoder.patch_embed1.proj = torch.nn.Conv2d(\n",
    "        in_channels=target_in_channels,\n",
    "        out_channels=old_conv.out_channels,\n",
    "        kernel_size=old_conv.kernel_size,\n",
    "        stride=old_conv.stride,\n",
    "        padding=old_conv.padding,\n",
    "        bias=old_conv.bias is not None\n",
    "    ).to(old_conv.weight.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.encoder.patch_embed1.proj.weight.copy_(new_weights)\n",
    "        if old_conv.bias is not None:\n",
    "            model.encoder.patch_embed1.proj.bias.copy_(old_conv.bias)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487981ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colorization_model = adapt_colorization_input(colorization_model, target_in_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410aacc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.encoder.load_state_dict(colorization_model.encoder.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a5d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.encoder.patch_embed1.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04734455",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = DiceLoss('multiclass', from_logits=True)\n",
    "\n",
    "class CombinedLoss(torch.nn.Module):\n",
    "    def __init__(self, dice_weight=0.5, focal_weight=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.dice_loss = DiceLoss('multiclass', from_logits=True)\n",
    "        self.focal_loss = FocalLoss('multiclass')\n",
    "        self.dice_weight = dice_weight\n",
    "        self.focal_weight = focal_weight\n",
    "    \n",
    "    def forward(self, outputs, targets):\n",
    "        dice_loss = self.dice_loss(outputs, targets)\n",
    "        focal_loss = self.focal_loss(outputs, targets)\n",
    "        return self.dice_weight * dice_loss + self.focal_weight * focal_loss\n",
    "\n",
    "def accuracy(yb, preds):\n",
    "    preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "    return (preds == yb).float().mean().item()\n",
    "\n",
    "def iou(yb, preds):\n",
    "    return calculate_iou[preds, yb]\n",
    "\n",
    "# def loss(yb, preds):\n",
    "#     loss = 0\n",
    "\n",
    "#     for pred in preds:\n",
    "#         loss += loss_fn(pred, yb)\n",
    "#     return loss\n",
    "\n",
    "metrics_callback = SegmentationMetricsCallback(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    common_metrics={'accuracy': accuracy},\n",
    "    class_metrics={\"iou\": calculate_iou}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cfba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = BlissLearner.BlissLearner(\n",
    "    model,\n",
    "    CombinedLoss(),\n",
    "    torch.optim.Adam,\n",
    "    dict(lr=1e-3),\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    callbacks=[\n",
    "        metrics_callback,\n",
    "        PrintCriteriaCallback()\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9259c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dea186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(learner._callback_state.epoch_train_loss['loss'])\n",
    "plt.plot(learner._callback_state.epoch_eval_loss['loss'])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7fd91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(learner._callback_state.epoch_train_criteria['iou_mean'])\n",
    "plt.plot(learner._callback_state.epoch_eval_criteria['iou_mean'])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef319d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(learner._callback_state.epoch_train_criteria['accuracy'])\n",
    "plt.plot(learner._callback_state.epoch_eval_criteria['accuracy'])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39355ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_inputs = torch.randn(1, 3, SIZE, SIZE).to(device)\n",
    "# onnx_program = torch.onnx.export(model, example_inputs, dynamo=True)\n",
    "# onnx_program.save(path_to_models + r\"\\UnetMITSegmentationModel.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6121bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), path_to_models + r\"\\UnetMITSegmentationModel.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a228cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.get_train_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5fd6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445814be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(xb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aaea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(valid_datasets)\n",
    "# N = 10\n",
    "fig, axs = plt.subplots(N, 3, figsize=(15, 120))\n",
    "model.eval()\n",
    "\n",
    "for i, (x, y) in enumerate(valid_datasets):\n",
    "    with torch.inference_mode():\n",
    "        inputs = torch.unsqueeze(x.to(device).float(), 0)\n",
    "        preds = model(inputs)[0]\n",
    "\n",
    "\n",
    "    print(preds.shape)\n",
    "    img = x.permute(1, 2, 0).cpu().numpy()\n",
    "    mask = y.cpu().numpy()\n",
    "    pred_mask = preds.argmax(dim=0).detach().cpu().numpy()\n",
    "    \n",
    "    axs[i, 0].imshow(img);\n",
    "    axs[i, 0].axis(False)\n",
    "    axs[i, 1].imshow(get_image_mask(mask));\n",
    "    axs[i, 1].axis(False)\n",
    "    axs[i, 2].imshow(get_image_mask(pred_mask));\n",
    "    axs[i, 2].axis(False)\n",
    "    if i == N - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa1e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c685fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
